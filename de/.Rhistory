library(readr)
base_path <- "C:/Users/ls68bino/Documents/GitHub/leostnbrk.github.io/data/kagglehub_cache/datasets/rajathmc/cornell-moviedialog-corpus/versions/1"
lines_path <- file.path(base_path, "movie_lines.txt")
df_lines <- read_delim(
lines_path,
delim = " +++$+++ ",
col_names = c("lineID", "characterID", "movieID", "character", "text"),
escape_double = FALSE,
trim_ws = TRUE
)
head(df_lines)
dim(df_lines)
View(df_lines)
View(df_lines)
# movie metadata
movies <- read_delim(
file.path(base_path, "movie_titles_metadata.txt"),
delim = " +++$+++ ",
col_names = c("movieID", "title", "year", "imdb_rating", "imdb_votes", "genres")
)
View(movies)
View(movies)
# characters
characters <- read_delim(
file.path(base_path, "movie_characters_metadata.txt"),
delim = " +++$+++ ",
col_names = c("characterID", "character", "movieID", "gender", "position")
)
View(characters)
View(characters)
# conversations
conversations <- read_delim(
file.path(base_path, "movie_conversations.txt"),
delim = " +++$+++ ",
col_names = c("characterID1", "characterID2", "movieID", "utteranceIDs")
)
View(conversations)
View(conversations)
# movie metadata
movies <- read_delim(
file.path(base_path, "movie_titles_metadata.txt"),
delim = " +++$+++ "
)
# characters
characters <- read_delim(
file.path(base_path, "movie_characters_metadata.txt"),
delim = " +++$+++ "
)
# conversations
conversations <- read_delim(
file.path(base_path, "movie_conversations.txt"),
delim = " +++$+++ "
)
# lines (already loaded before)
lines <- df_lines
df_lines <- read_delim(
lines_path,
delim = " +++$+++ ",
escape_double = FALSE,
trim_ws = TRUE
)
head(df_lines)
dim(df_lines)
# movie metadata
movies <- read_delim(
file.path(base_path, "movie_titles_metadata.txt"),
delim = " +++$+++ "
)
# characters
characters <- read_delim(
file.path(base_path, "movie_characters_metadata.txt"),
delim = " +++$+++ "
)
# conversations
conversations <- read_delim(
file.path(base_path, "movie_conversations.txt"),
delim = " +++$+++ "
)
# lines (already loaded before)
lines <- df_lines
View(conversations)
View(conversations)
View(df_lines)
View(df_lines)
View(conversations)
View(conversations)
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
base_path <- "C:/Users/ls68bino/Documents/GitHub/leostnbrk.github.io/data/kagglehub_cache/datasets/rajathmc/cornell-moviedialog-corpus/versions/1"
delim_str <- " +++$+++ "
# 1) Movie titles metadata
movies <- read_delim(
file.path(base_path, "movie_titles_metadata.txt"),
delim = delim_str,
col_names = c(
"movieID",
"title",
"year",
"imdb_rating",
"imdb_votes",
"genres"
),
trim_ws = TRUE
)
View(movies)
View(movies)
# 2) Character metadata
characters <- read_delim(
file.path(base_path, "movie_characters_metadata.txt"),
delim = delim_str,
col_names = c(
"characterID",
"character",
"movieID",
"movie_title",
"gender",
"credit_position"
),
trim_ws = TRUE
)
View(characters)
View(characters)
# 3) Lines (utterances)
lines <- read_delim(
file.path(base_path, "movie_lines.txt"),
delim = delim_str,
col_names = c(
"lineID",
"characterID",
"movieID",
"character",
"text"
),
trim_ws = TRUE
)
View(lines)
View(lines)
# 4) Conversations
conversations <- read_delim(
file.path(base_path, "movie_conversations.txt"),
delim = delim_str,
col_names = c(
"characterID_1",
"characterID_2",
"movieID",
"utteranceIDs"
),
trim_ws = TRUE
)
View(conversations)
View(conversations)
# 5) Raw script URLs
raw_urls <- read_delim(
file.path(base_path, "raw_script_urls.txt"),
delim = delim_str,
col_names = c(
"movieID",
"script_url"
),
trim_ws = TRUE
)
View(raw_urls)
View(raw_urls)
# Optional: conversations â†’ long (chronological utterances)
conversations_long <- conversations %>%
mutate(utteranceIDs = str_remove_all(utteranceIDs, "[\\[\\]' ]")) %>%
separate_rows(utteranceIDs, sep = ",") %>%
rename(lineID = utteranceIDs)
# Optional: fully joined dataset
full_data <- conversations_long %>%
left_join(lines, by = "lineID") %>%
left_join(characters, by = c("characterID", "movieID")) %>%
left_join(movies, by = "movieID")
full_data <- conversations_long %>%
left_join(lines, by = "lineID") %>%                 # adds characterID + movieID
left_join(characters, by = c("characterID", "movieID")) %>%
left_join(movies, by = "movieID")
full_data <- conversations_long %>%
left_join(lines, by = "lineID") %>%
select(-movieID.x) %>%
rename(movieID = movieID.y) %>%
left_join(characters, by = c("characterID", "movieID")) %>%
left_join(movies, by = "movieID")
View(full_data)
View(full_data)
load_kaggle_data <- function(local_filename, kaggle_dataset) {
if (!file.exists(local_filename)) {
# Load Kaggle API credentials only if the file needs to be downloaded
kaggle_credentials <- fromJSON("./nogit_kaggle.json")
kaggle_username <- kaggle_credentials$username
kaggle_key <- kaggle_credentials$key
Sys.setenv(KAGGLE_USERNAME = kaggle_username)
Sys.setenv(KAGGLE_KEY = kaggle_key)
if (system("kaggle --version", ignore.stdout = TRUE, ignore.stderr = TRUE) != 0) {
stop("Kaggle CLI is not installed or not in the PATH.")
}
message(paste("Downloading", local_filename, "from Kaggle"))
if (system(paste("kaggle datasets download", kaggle_dataset, "-f", local_filename)) != 0) {
stop("Failed to download the dataset with Kaggle CLI.")
}
unzip_status <- system(paste("unzip -o", local_filename))
if (unzip_status != 0) {
stop("Failed to unzip the dataset.")
}
zip_filename <- paste0(local_filename, ".zip")
if (file.exists(zip_filename)) {
file.remove(zip_filename)
}
} else {
message(paste(local_filename, "found locally. Loading..."))
}
read_csv(local_filename)
}
k_dataset <- "goyaladi/twitter-dataset"
local_wide <- "merged_stack_wide.csv"
# Load the dataset
merged_stack_wide <- load_kaggle_data(local_wide, k_dataset)
library(jsonlite)
library(readr)
library(tidyverse)
# Load the dataset
merged_stack_wide <- load_kaggle_data(local_wide, k_dataset)
library(jsonlite)
library(dplyr)
# Folder containing your JSON files
json_folder <- "C:\Users\ls68bino\Documents\GitHub\leostnbrk.github.io\data\presidential speeches\speeches"
library(jsonlite)
library(dplyr)
# Folder containing your JSON files
json_folder <- "C:\\Users\\ls68bino\\Documents\\GitHub\\leostnbrk.github.io\\data\\presidential speeches\\speeches"
# List all JSON files in the folder
json_files <- list.files(json_folder, pattern = "\\.json$", full.names = TRUE)
# Function to read a JSON and convert it to a named list
read_json_as_list <- function(file) {
fromJSON(file, simplifyVector = FALSE) # keep as list
}
# Read all JSONs
json_data <- lapply(json_files, read_json_as_list)
# Identify all unique keys across all JSONs
all_keys <- unique(unlist(lapply(json_data, names)))
# Convert each JSON list to a dataframe, ensuring all columns exist
json_dfs <- lapply(json_data, function(x) {
# Add missing keys as NA
missing_keys <- setdiff(all_keys, names(x))
if (length(missing_keys) > 0) {
x[missing_keys] <- NA
}
as.data.frame(x, stringsAsFactors = FALSE)
})
# Combine all into one dataframe
final_df <- bind_rows(json_dfs)
# View result
print(final_df)
View(final_df)
View(final_df)
# View result
print(head(final_df))
# View result (first row)
head(final_df, 1)
